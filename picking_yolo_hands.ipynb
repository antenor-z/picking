{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ultralytics in d:\\puc\\topicos_ciencia_dados_visaocomputacional\\picking\\.venv\\lib\\site-packages (8.2.18)\n",
      "Requirement already satisfied: matplotlib>=3.3.0 in d:\\puc\\topicos_ciencia_dados_visaocomputacional\\picking\\.venv\\lib\\site-packages (from ultralytics) (3.9.0)\n",
      "Requirement already satisfied: opencv-python>=4.6.0 in d:\\puc\\topicos_ciencia_dados_visaocomputacional\\picking\\.venv\\lib\\site-packages (from ultralytics) (4.9.0.80)\n",
      "Requirement already satisfied: pillow>=7.1.2 in d:\\puc\\topicos_ciencia_dados_visaocomputacional\\picking\\.venv\\lib\\site-packages (from ultralytics) (10.3.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in d:\\puc\\topicos_ciencia_dados_visaocomputacional\\picking\\.venv\\lib\\site-packages (from ultralytics) (6.0.1)\n",
      "Requirement already satisfied: requests>=2.23.0 in d:\\puc\\topicos_ciencia_dados_visaocomputacional\\picking\\.venv\\lib\\site-packages (from ultralytics) (2.31.0)\n",
      "Requirement already satisfied: scipy>=1.4.1 in d:\\puc\\topicos_ciencia_dados_visaocomputacional\\picking\\.venv\\lib\\site-packages (from ultralytics) (1.13.0)\n",
      "Requirement already satisfied: torch>=1.8.0 in d:\\puc\\topicos_ciencia_dados_visaocomputacional\\picking\\.venv\\lib\\site-packages (from ultralytics) (2.3.0)\n",
      "Requirement already satisfied: torchvision>=0.9.0 in d:\\puc\\topicos_ciencia_dados_visaocomputacional\\picking\\.venv\\lib\\site-packages (from ultralytics) (0.18.0)\n",
      "Requirement already satisfied: tqdm>=4.64.0 in d:\\puc\\topicos_ciencia_dados_visaocomputacional\\picking\\.venv\\lib\\site-packages (from ultralytics) (4.66.4)\n",
      "Requirement already satisfied: psutil in d:\\puc\\topicos_ciencia_dados_visaocomputacional\\picking\\.venv\\lib\\site-packages (from ultralytics) (5.9.8)\n",
      "Requirement already satisfied: py-cpuinfo in d:\\puc\\topicos_ciencia_dados_visaocomputacional\\picking\\.venv\\lib\\site-packages (from ultralytics) (9.0.0)\n",
      "Requirement already satisfied: thop>=0.1.1 in d:\\puc\\topicos_ciencia_dados_visaocomputacional\\picking\\.venv\\lib\\site-packages (from ultralytics) (0.1.1.post2209072238)\n",
      "Requirement already satisfied: pandas>=1.1.4 in d:\\puc\\topicos_ciencia_dados_visaocomputacional\\picking\\.venv\\lib\\site-packages (from ultralytics) (2.2.2)\n",
      "Requirement already satisfied: seaborn>=0.11.0 in d:\\puc\\topicos_ciencia_dados_visaocomputacional\\picking\\.venv\\lib\\site-packages (from ultralytics) (0.13.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in d:\\puc\\topicos_ciencia_dados_visaocomputacional\\picking\\.venv\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in d:\\puc\\topicos_ciencia_dados_visaocomputacional\\picking\\.venv\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in d:\\puc\\topicos_ciencia_dados_visaocomputacional\\picking\\.venv\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in d:\\puc\\topicos_ciencia_dados_visaocomputacional\\picking\\.venv\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (1.4.5)\n",
      "Requirement already satisfied: numpy>=1.23 in d:\\puc\\topicos_ciencia_dados_visaocomputacional\\picking\\.venv\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\puc\\topicos_ciencia_dados_visaocomputacional\\picking\\.venv\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (24.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in d:\\puc\\topicos_ciencia_dados_visaocomputacional\\picking\\.venv\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in d:\\puc\\topicos_ciencia_dados_visaocomputacional\\picking\\.venv\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\puc\\topicos_ciencia_dados_visaocomputacional\\picking\\.venv\\lib\\site-packages (from pandas>=1.1.4->ultralytics) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\puc\\topicos_ciencia_dados_visaocomputacional\\picking\\.venv\\lib\\site-packages (from pandas>=1.1.4->ultralytics) (2024.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\puc\\topicos_ciencia_dados_visaocomputacional\\picking\\.venv\\lib\\site-packages (from requests>=2.23.0->ultralytics) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\puc\\topicos_ciencia_dados_visaocomputacional\\picking\\.venv\\lib\\site-packages (from requests>=2.23.0->ultralytics) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\puc\\topicos_ciencia_dados_visaocomputacional\\picking\\.venv\\lib\\site-packages (from requests>=2.23.0->ultralytics) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\puc\\topicos_ciencia_dados_visaocomputacional\\picking\\.venv\\lib\\site-packages (from requests>=2.23.0->ultralytics) (2024.2.2)\n",
      "Requirement already satisfied: filelock in d:\\puc\\topicos_ciencia_dados_visaocomputacional\\picking\\.venv\\lib\\site-packages (from torch>=1.8.0->ultralytics) (3.14.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in d:\\puc\\topicos_ciencia_dados_visaocomputacional\\picking\\.venv\\lib\\site-packages (from torch>=1.8.0->ultralytics) (4.11.0)\n",
      "Requirement already satisfied: sympy in d:\\puc\\topicos_ciencia_dados_visaocomputacional\\picking\\.venv\\lib\\site-packages (from torch>=1.8.0->ultralytics) (1.12)\n",
      "Requirement already satisfied: networkx in d:\\puc\\topicos_ciencia_dados_visaocomputacional\\picking\\.venv\\lib\\site-packages (from torch>=1.8.0->ultralytics) (3.3)\n",
      "Requirement already satisfied: jinja2 in d:\\puc\\topicos_ciencia_dados_visaocomputacional\\picking\\.venv\\lib\\site-packages (from torch>=1.8.0->ultralytics) (3.1.4)\n",
      "Requirement already satisfied: fsspec in d:\\puc\\topicos_ciencia_dados_visaocomputacional\\picking\\.venv\\lib\\site-packages (from torch>=1.8.0->ultralytics) (2024.5.0)\n",
      "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in d:\\puc\\topicos_ciencia_dados_visaocomputacional\\picking\\.venv\\lib\\site-packages (from torch>=1.8.0->ultralytics) (2021.4.0)\n",
      "Requirement already satisfied: colorama in d:\\puc\\topicos_ciencia_dados_visaocomputacional\\picking\\.venv\\lib\\site-packages (from tqdm>=4.64.0->ultralytics) (0.4.6)\n",
      "Requirement already satisfied: intel-openmp==2021.* in d:\\puc\\topicos_ciencia_dados_visaocomputacional\\picking\\.venv\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=1.8.0->ultralytics) (2021.4.0)\n",
      "Requirement already satisfied: tbb==2021.* in d:\\puc\\topicos_ciencia_dados_visaocomputacional\\picking\\.venv\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=1.8.0->ultralytics) (2021.12.0)\n",
      "Requirement already satisfied: six>=1.5 in d:\\puc\\topicos_ciencia_dados_visaocomputacional\\picking\\.venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\puc\\topicos_ciencia_dados_visaocomputacional\\picking\\.venv\\lib\\site-packages (from jinja2->torch>=1.8.0->ultralytics) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in d:\\puc\\topicos_ciencia_dados_visaocomputacional\\picking\\.venv\\lib\\site-packages (from sympy->torch>=1.8.0->ultralytics) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "from yolov9.detect import run as run_yolo_detect\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "from tkinter import Tk, simpledialog\n",
    "import mediapipe as mp\n",
    "import threading\n",
    "import json\n",
    "import cv2\n",
    "import json\n",
    "import time\n",
    "\n",
    "!pip install ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_yolov9():\n",
    "    for file in Path(\"data/frame\").glob(\"*.jpg\"):\n",
    "        run_yolo_detect(weights=\"yolo_weights/weights/last.pt\",\n",
    "                        source=str(file),\n",
    "                        device=\"cpu\",\n",
    "                        conf_thres=0.1,\n",
    "                        save_txt=True,\n",
    "                        project=\"data/detect_yolov9\")\n",
    "\n",
    "def extract_output_json():\n",
    "    data_dict = {}\n",
    "    for file in Path(\"data/detect_yolov9\").glob(\"**/*.txt\"):\n",
    "        filename = file.name.split(\".\")[0]\n",
    "        data_dict[filename] = {}\n",
    "        object_detected = 0\n",
    "        with file.open(mode=\"r\") as f:\n",
    "            for line in f.readlines():\n",
    "                data_line = line.replace(\"\\n\", \"\").split(\" \")\n",
    "                object_name = f\"object{object_detected}_{data_line[0]}\"\n",
    "                data_dict[filename][object_name] = data_line[1:]\n",
    "                object_detected += 1\n",
    "    with Path(\"data/output.json\").open(mode=\"w+\") as f:\n",
    "        json.dump(data_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLO  6b38221 Python-3.11.9 torch-2.3.0+cpu CPU\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model summary (fused): 168 layers, 11127906 parameters, 0 gradients, 28.4 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "image 1/1 D:\\PUC\\Topicos_Ciencia_Dados_VisaoComputacional\\picking\\data\\frame\\capacitor ceramico 22pF_4.jpg: 480x640 (no detections), 94.5ms\n",
      "Speed: 1.0ms pre-process, 94.5ms inference, 0.0ms NMS per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mdata\\detect_yolov9\\exp9\u001b[0m\n",
      "0 labels saved to data\\detect_yolov9\\exp9\\labels\n",
      "YOLO  6b38221 Python-3.11.9 torch-2.3.0+cpu CPU\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model summary (fused): 168 layers, 11127906 parameters, 0 gradients, 28.4 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "image 1/1 D:\\PUC\\Topicos_Ciencia_Dados_VisaoComputacional\\picking\\data\\frame\\diodo_6.jpg: 480x640 1 botao, 87.5ms\n",
      "Speed: 0.0ms pre-process, 87.5ms inference, 0.0ms NMS per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mdata\\detect_yolov9\\exp10\u001b[0m\n",
      "1 labels saved to data\\detect_yolov9\\exp10\\labels\n",
      "YOLO  6b38221 Python-3.11.9 torch-2.3.0+cpu CPU\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model summary (fused): 168 layers, 11127906 parameters, 0 gradients, 28.4 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "image 1/1 D:\\PUC\\Topicos_Ciencia_Dados_VisaoComputacional\\picking\\data\\frame\\display OLED_1.jpg: 480x640 1 display lcd, 1 microcontrolador, 84.5ms\n",
      "Speed: 1.0ms pre-process, 84.5ms inference, 1.0ms NMS per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mdata\\detect_yolov9\\exp11\u001b[0m\n",
      "1 labels saved to data\\detect_yolov9\\exp11\\labels\n",
      "YOLO  6b38221 Python-3.11.9 torch-2.3.0+cpu CPU\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model summary (fused): 168 layers, 11127906 parameters, 0 gradients, 28.4 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "image 1/1 D:\\PUC\\Topicos_Ciencia_Dados_VisaoComputacional\\picking\\data\\frame\\led_ext_4.jpg: 384x640 3 leds, 82.5ms\n",
      "Speed: 1.0ms pre-process, 82.5ms inference, 1.0ms NMS per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mdata\\detect_yolov9\\exp12\u001b[0m\n",
      "1 labels saved to data\\detect_yolov9\\exp12\\labels\n",
      "YOLO  6b38221 Python-3.11.9 torch-2.3.0+cpu CPU\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model summary (fused): 168 layers, 11127906 parameters, 0 gradients, 28.4 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "image 1/1 D:\\PUC\\Topicos_Ciencia_Dados_VisaoComputacional\\picking\\data\\frame\\led_resistor_ext.jpg: 416x640 1 botao, 1 led, 1 resistor, 84.5ms\n",
      "Speed: 1.0ms pre-process, 84.5ms inference, 0.0ms NMS per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mdata\\detect_yolov9\\exp13\u001b[0m\n",
      "1 labels saved to data\\detect_yolov9\\exp13\\labels\n",
      "YOLO  6b38221 Python-3.11.9 torch-2.3.0+cpu CPU\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model summary (fused): 168 layers, 11127906 parameters, 0 gradients, 28.4 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "image 1/1 D:\\PUC\\Topicos_Ciencia_Dados_VisaoComputacional\\picking\\data\\frame\\microcontrolador ATTiny85_6.jpg: 480x640 1 microcontrolador, 85.5ms\n",
      "Speed: 1.0ms pre-process, 85.5ms inference, 0.0ms NMS per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mdata\\detect_yolov9\\exp14\u001b[0m\n",
      "1 labels saved to data\\detect_yolov9\\exp14\\labels\n",
      "YOLO  6b38221 Python-3.11.9 torch-2.3.0+cpu CPU\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model summary (fused): 168 layers, 11127906 parameters, 0 gradients, 28.4 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "image 1/1 D:\\PUC\\Topicos_Ciencia_Dados_VisaoComputacional\\picking\\data\\frame\\resistor_11.jpg: 480x640 (no detections), 93.5ms\n",
      "Speed: 1.0ms pre-process, 93.5ms inference, 0.0ms NMS per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mdata\\detect_yolov9\\exp15\u001b[0m\n",
      "0 labels saved to data\\detect_yolov9\\exp15\\labels\n",
      "YOLO  6b38221 Python-3.11.9 torch-2.3.0+cpu CPU\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model summary (fused): 168 layers, 11127906 parameters, 0 gradients, 28.4 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "image 1/1 D:\\PUC\\Topicos_Ciencia_Dados_VisaoComputacional\\picking\\data\\frame\\resistor_led_ext3.jpg: 384x640 2 leds, 2 resistors, 83.5ms\n",
      "Speed: 1.0ms pre-process, 83.5ms inference, 1.0ms NMS per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mdata\\detect_yolov9\\exp16\u001b[0m\n",
      "1 labels saved to data\\detect_yolov9\\exp16\\labels\n",
      "YOLO  6b38221 Python-3.11.9 torch-2.3.0+cpu CPU\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model summary (fused): 168 layers, 11127906 parameters, 0 gradients, 28.4 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "image 1/1 D:\\PUC\\Topicos_Ciencia_Dados_VisaoComputacional\\picking\\data\\frame\\teste12_0.jpg: 352x640 (no detections), 81.5ms\n",
      "Speed: 0.0ms pre-process, 81.5ms inference, 0.0ms NMS per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mdata\\detect_yolov9\\exp17\u001b[0m\n",
      "0 labels saved to data\\detect_yolov9\\exp17\\labels\n",
      "YOLO  6b38221 Python-3.11.9 torch-2.3.0+cpu CPU\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model summary (fused): 168 layers, 11127906 parameters, 0 gradients, 28.4 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "image 1/1 D:\\PUC\\Topicos_Ciencia_Dados_VisaoComputacional\\picking\\data\\frame\\teste12_10.jpg: 352x640 (no detections), 82.5ms\n",
      "Speed: 1.0ms pre-process, 82.5ms inference, 0.0ms NMS per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mdata\\detect_yolov9\\exp18\u001b[0m\n",
      "0 labels saved to data\\detect_yolov9\\exp18\\labels\n",
      "YOLO  6b38221 Python-3.11.9 torch-2.3.0+cpu CPU\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model summary (fused): 168 layers, 11127906 parameters, 0 gradients, 28.4 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "image 1/1 D:\\PUC\\Topicos_Ciencia_Dados_VisaoComputacional\\picking\\data\\frame\\teste12_20.jpg: 352x640 (no detections), 79.5ms\n",
      "Speed: 1.0ms pre-process, 79.5ms inference, 0.0ms NMS per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mdata\\detect_yolov9\\exp19\u001b[0m\n",
      "0 labels saved to data\\detect_yolov9\\exp19\\labels\n",
      "YOLO  6b38221 Python-3.11.9 torch-2.3.0+cpu CPU\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model summary (fused): 168 layers, 11127906 parameters, 0 gradients, 28.4 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "image 1/1 D:\\PUC\\Topicos_Ciencia_Dados_VisaoComputacional\\picking\\data\\frame\\teste12_30.jpg: 352x640 (no detections), 85.5ms\n",
      "Speed: 0.0ms pre-process, 85.5ms inference, 1.0ms NMS per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mdata\\detect_yolov9\\exp20\u001b[0m\n",
      "0 labels saved to data\\detect_yolov9\\exp20\\labels\n",
      "YOLO  6b38221 Python-3.11.9 torch-2.3.0+cpu CPU\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model summary (fused): 168 layers, 11127906 parameters, 0 gradients, 28.4 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "image 1/1 D:\\PUC\\Topicos_Ciencia_Dados_VisaoComputacional\\picking\\data\\frame\\teste12_40.jpg: 352x640 (no detections), 83.0ms\n",
      "Speed: 1.0ms pre-process, 83.0ms inference, 0.0ms NMS per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mdata\\detect_yolov9\\exp21\u001b[0m\n",
      "0 labels saved to data\\detect_yolov9\\exp21\\labels\n",
      "YOLO  6b38221 Python-3.11.9 torch-2.3.0+cpu CPU\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model summary (fused): 168 layers, 11127906 parameters, 0 gradients, 28.4 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "image 1/1 D:\\PUC\\Topicos_Ciencia_Dados_VisaoComputacional\\picking\\data\\frame\\teste12_50.jpg: 352x640 (no detections), 78.5ms\n",
      "Speed: 0.0ms pre-process, 78.5ms inference, 0.0ms NMS per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mdata\\detect_yolov9\\exp22\u001b[0m\n",
      "0 labels saved to data\\detect_yolov9\\exp22\\labels\n",
      "YOLO  6b38221 Python-3.11.9 torch-2.3.0+cpu CPU\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model summary (fused): 168 layers, 11127906 parameters, 0 gradients, 28.4 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "image 1/1 D:\\PUC\\Topicos_Ciencia_Dados_VisaoComputacional\\picking\\data\\frame\\teste12_60.jpg: 352x640 (no detections), 75.5ms\n",
      "Speed: 0.0ms pre-process, 75.5ms inference, 0.0ms NMS per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mdata\\detect_yolov9\\exp23\u001b[0m\n",
      "0 labels saved to data\\detect_yolov9\\exp23\\labels\n",
      "YOLO  6b38221 Python-3.11.9 torch-2.3.0+cpu CPU\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model summary (fused): 168 layers, 11127906 parameters, 0 gradients, 28.4 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "image 1/1 D:\\PUC\\Topicos_Ciencia_Dados_VisaoComputacional\\picking\\data\\frame\\teste12_70.jpg: 352x640 (no detections), 73.5ms\n",
      "Speed: 0.0ms pre-process, 73.5ms inference, 0.0ms NMS per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mdata\\detect_yolov9\\exp24\u001b[0m\n",
      "0 labels saved to data\\detect_yolov9\\exp24\\labels\n",
      "YOLO  6b38221 Python-3.11.9 torch-2.3.0+cpu CPU\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model summary (fused): 168 layers, 11127906 parameters, 0 gradients, 28.4 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "image 1/1 D:\\PUC\\Topicos_Ciencia_Dados_VisaoComputacional\\picking\\data\\frame\\teste12_80.jpg: 352x640 (no detections), 71.5ms\n",
      "Speed: 1.0ms pre-process, 71.5ms inference, 0.0ms NMS per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mdata\\detect_yolov9\\exp25\u001b[0m\n",
      "0 labels saved to data\\detect_yolov9\\exp25\\labels\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "TOP_BORDER_HEIGHT = 80\n",
    "CONFIG_PATH = \"config-example.json\"\n",
    "CAM_NUMBER = 0\n",
    "\n",
    "new_object = None\n",
    "def get_new_object(rect_start, rect_end):\n",
    "    global new_object\n",
    "    root = Tk()\n",
    "    root.withdraw()\n",
    "    new_object_name = simpledialog.askstring(\"Novo objeto\", \"Escolha um nome:\")\n",
    "    root.destroy()\n",
    "    new_object = {\n",
    "        \"name\": new_object_name,\n",
    "        \"point_1\": [rect_start[0], rect_start[1] - TOP_BORDER_HEIGHT], \n",
    "        \"point_2\": [rect_end[0], rect_end[1] - TOP_BORDER_HEIGHT]\n",
    "    }\n",
    "\n",
    "def mid_point(acc_cx, acc_cy):\n",
    "    avg_cx = int(sum(acc_cx) / len(acc_cx))\n",
    "    avg_cy = int(sum(acc_cy) / len(acc_cy))\n",
    "    return [avg_cx, avg_cy]\n",
    "\n",
    "try:\n",
    "    with open(CONFIG_PATH, 'r') as fp:\n",
    "        points_of_interest = json.load(fp)\n",
    "except FileNotFoundError:\n",
    "    with open(CONFIG_PATH, 'w+') as fp:\n",
    "        fp.write(\"[]\")\n",
    "        points_of_interest = []\n",
    "\n",
    "drawing = False\n",
    "ix,iy = -1,-1\n",
    "def draw_rectangle(event, x, y, flags, param):\n",
    "    global drawing, rect_start, points_of_interest\n",
    "    if event == cv2.EVENT_LBUTTONDOWN:\n",
    "        drawing = True\n",
    "        rect_start = (x, y)\n",
    "    elif event == cv2.EVENT_MOUSEMOVE:\n",
    "        if drawing:\n",
    "            img_copy = img.copy()\n",
    "            cv2.rectangle(img_copy, rect_start, (x, y), (0, 255, 0), 1)\n",
    "            cv2.imshow(\"TOP CIENCIA DE DADOS I\", img_copy)\n",
    "    elif event == cv2.EVENT_LBUTTONUP:\n",
    "        drawing = False\n",
    "        rect_end = (x, y)\n",
    "        cv2.rectangle(img, rect_start, rect_end, (0, 255, 0), 1)\n",
    "        cv2.imshow(\"TOP CIENCIA DE DADOS I\", img)\n",
    "        threading.Thread(target=get_new_object, args=(rect_start, rect_end)).start()        \n",
    "\n",
    "cap = cv2.VideoCapture(CAM_NUMBER)\n",
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands()\n",
    "mp_draw = mp.solutions.drawing_utils\n",
    "\n",
    "cv2.namedWindow(\"TOP CIENCIA DE DADOS I\")\n",
    "cv2.setMouseCallback(\"TOP CIENCIA DE DADOS I\", draw_rectangle)\n",
    "currentframe = 0\n",
    "try:\n",
    "    while True:\n",
    "        hands_positions = []\n",
    "        matches = []\n",
    "\n",
    "        success, img = cap.read()\n",
    "\n",
    "        img = cv2.flip(img, 1)\n",
    "        imgRGB = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        results = hands.process(imgRGB)\n",
    "\n",
    "        if results.multi_hand_landmarks:\n",
    "            hands_positions = []  # reset list\n",
    "            for handLms in results.multi_hand_landmarks:\n",
    "                acc_cx = []\n",
    "                acc_cy = []\n",
    "                for id, lm in enumerate(handLms.landmark):\n",
    "                    h, w, c = img.shape\n",
    "                    cx, cy = int(lm.x * w), int(lm.y * h)\n",
    "                    acc_cx, acc_cy = acc_cx + [cx], acc_cy + [cy]\n",
    "\n",
    "                (avg_cx, avg_cy) = mid_point(acc_cx, acc_cy)\n",
    "                cv2.circle(img, (avg_cx, avg_cy), 2, (255, 0, 0), 20, cv2.FILLED)\n",
    "                hands_positions.append([avg_cx, avg_cy])\n",
    "                mp_draw.draw_landmarks(img, handLms, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "        for location in points_of_interest:\n",
    "            color = (200, 0, 0)\n",
    "            for hand_position in hands_positions:\n",
    "                if location[\"point_1\"][0] < hand_position[0] < location[\"point_2\"][0] and \\\n",
    "                    location[\"point_1\"][1] < hand_position[1] < location[\"point_2\"][1]:\n",
    "                    matches.append(location[\"name\"])\n",
    "                    color = (0, 200, 0)\n",
    "\n",
    "                    name = './data/frame/' + str(location[\"name\"]) + \"_\" + str(currentframe) + '.jpg'\n",
    "                    roi = img[166:959,24:1433]\n",
    "                    if currentframe % 10 == 0:\n",
    "                        cv2.imwrite(name, roi)\n",
    "                        threading.Thread(target=process_yolov9, args=(name,))\n",
    "                    currentframe += 1\n",
    "\n",
    "            title_position = (int(location[\"point_1\"][0]), int(location[\"point_1\"][1] - 10))\n",
    "            cv2.putText(img, location[\"name\"], title_position, cv2.QT_FONT_NORMAL, 0.8, color, 1)\n",
    "            cv2.rectangle(img, location[\"point_1\"], location[\"point_2\"], color, 1)\n",
    "\n",
    "        img = cv2.copyMakeBorder(img, TOP_BORDER_HEIGHT, 0, 0, 0, cv2.BORDER_CONSTANT, value=(0, 0, 0))\n",
    "        cv2.putText(img, \" \".join(matches), (10, 40), cv2.QT_FONT_NORMAL, 0.7, (0, 255, 0), 1)\n",
    "        cv2.putText(img, \"Use o mouse para desenhar um retangulo em volta do objeto\", (10, 15), cv2.QT_FONT_NORMAL, 0.5, (100, 100, 100), 1)\n",
    "        cv2.putText(img, str(hands_positions or \"\"), (10, 65), cv2.QT_FONT_NORMAL, 0.7, (0, 255, 0), 1)\n",
    "\n",
    "        cv2.imshow(\"TOP CIENCIA DE DADOS I\", img)\n",
    "\n",
    "        if new_object is not None:\n",
    "            if new_object[\"name\"] is not None and new_object[\"name\"].strip() != \"\":\n",
    "                points_of_interest.append(new_object)\n",
    "                with open(CONFIG_PATH, \"w\") as fp:\n",
    "                    json.dump(points_of_interest, fp=fp, indent=4)\n",
    "                print(f\"New object {new_object} added.\")\n",
    "            new_object = None\n",
    "\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        if key == ord(\"q\") or cv2.getWindowProperty(\"TOP CIENCIA DE DADOS I\", cv2.WND_PROP_VISIBLE) < 1:\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    process_yolov9()\n",
    "    extract_output_json()\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
